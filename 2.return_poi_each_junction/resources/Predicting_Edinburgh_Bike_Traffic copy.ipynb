{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PREDICTING EDINBURGH BIKE TRAFFIC\n",
    "\n",
    "As cities around the world seek to ensure bikes account for an increasing proportion of journeys, forecasting fluctuations in bicycle use due to weather changes will become more and more important to the-day-to day planning of public transport provisioning.\n",
    "\n",
    "Our objective is to use historical data from [Edinburgh's bike counters](http://www.edinburghopendata.info/dataset/bike-counter-data-set-cluster) along with [weather data](http://www.ed.ac.uk/geosciences/weather-station) gathered by the Geosciences Dep. at the University of Edinburgh to build two models capable of forecasting bike traffic based on short-term weather forecasts.\n",
    "\n",
    "A model of this type has the potential to assist cities in the day to day running of their public transport infrastructure through better forecasting of demand fluctuations for public transport as a result of people choosing to use / not use their bikes.\n",
    "\n",
    "---\n",
    "**Libraries needed to run this workbook & version installed:**\n",
    "```python\n",
    "IPython 4.0.0-9\n",
    "pandas 0.19.2-2\n",
    "numpy 1.11.3-2\n",
    "matplotlib 1.5.1-12\n",
    "sqlalchemy 1.1.6-1\n",
    "sklearn 0.18.1\n",
    "statsmodels 0.8.0-1\n",
    "pyflux 0.4.14\n",
    "```\n",
    "\n",
    "*This workbook requires a connection to the UCL network*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I. BIKE DATA, STORAGE, CONSISTENCY AND SAMPLE SELECTION](#I. BIKE DATA, STORAGE, CONSISTENCY AND SAMPLE SELECTION])\n",
    "<a href='I. BIKE DATA, STORAGE, CONSISTENCY AND SAMPLE SELECTION'></a>\n",
    "\n",
    "    I.A) THE BIKE DATA\n",
    "    \n",
    "    I.B) BIKE DATA STORAGE\n",
    "    \n",
    "    I.C) BIKE DATA CONSISTENCY AND SAMPLE SELECTION\n",
    "\n",
    "[II. WEATHER DATA, STORAGE, CLEANING, CONSISTENCY AND SAMPLE SELECTION](#II. WEATHER DATA, STORAGE, CLEANING, CONSISTENCY AND SAMPLE SELECTION)\n",
    "<a href='II. WEATHER DATA, STORAGE, CLEANING, CONSISTENCY AND SAMPLE SELECTION'></a>\n",
    "\n",
    "    II.A) THE WEATHER DATA\n",
    "    \n",
    "    II.B) BIKE DATA STORAGE\n",
    "    \n",
    "    II.C) WEATHER DATA CLEANING\n",
    "    \n",
    "    II.D) WEATHER DATA CONSISTENCY AND SAMPLE SELECTION\n",
    "\n",
    "[III. DATA ANALYSIS](#III. DATA ANALYSIS)\n",
    "<a href='III. DATA ANALYSIS'></a>\n",
    "\n",
    "    III.A) PRELIMINARY DATA ANALYSIS\n",
    "    \n",
    "    III.B) SEASONALITY AND TIME LAGS\n",
    "    \n",
    "[IV. FORECASTING WITH SCIKIT LEARN](#IV. FORECASTING WITH SCIKIT LEARN)\n",
    "<a href='IV. FORECASTING WITH SCIKIT LEARN'></a>\n",
    "\n",
    "[V. FORECASTING WITH ARIMAX](#V. FORECASTING WITH ARIMAX)\n",
    "<a href='V. FORECASTING WITH ARIMAX'></a>\n",
    "\n",
    "[DISCUSSION](#DISCUSSION)\n",
    "<a href='DISCUSSION'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle ON/OFF the Input Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle ON/OFF the Input Code\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Loaded.\n",
      "Connected to Database.\n"
     ]
    }
   ],
   "source": [
    "#### IMPORT LIBRARIES ####\n",
    "# import libraries, and set pd as the pandas alias\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\n",
    "# numpy\n",
    "import numpy as np\n",
    "# this one ensures graphs properly display in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15,6\n",
    "rcParams.update({'font.size': 15})\n",
    "rcParams['xtick.labelsize'] = 13\n",
    "# to display multiple outputs in one output window\n",
    "from IPython.display import display\n",
    "# to use markdown in print statements\n",
    "from IPython.display import Markdown\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "#for sqrt function etc...\n",
    "import math\n",
    "#for date function\n",
    "import datetime\n",
    "\n",
    "print 'Packages Loaded.'\n",
    "\n",
    "            #### CREATE SQL DATABASE CONNECTION ####\n",
    "# import the SQLAlchemy libraries ('pymysql' package needs to be installed)\n",
    "from sqlalchemy import create_engine\n",
    "# create the connection string to the MySQL database\n",
    "engine = create_engine('mysql+pymysql://zcsah83:buciyutipa@128.40.150.34:3306/zcsah83')\n",
    "# create connection to the database\n",
    "conn = engine\n",
    "\n",
    "print 'Connected to Database.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='I. BIKE DATA, STORAGE, CONSISTENCY AND SAMPLE SELECTION'></a>\n",
    "# I. BIKE DATA, STORAGE, CONSISTENCY AND SAMPLE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.A) THE BIKE DATA\n",
    "\n",
    "Hourly traffic counts from 48 bicycle counters installed around Edinburgh were downloaded from the [City's Open Data portal](http://www.edinburghopendata.info/dataset/bike-counter-data-set-cluster).\n",
    "\n",
    "Although, exact locations of the counters could not be obtained, we determined their approximate locations:\n",
    "\n",
    "![Imgur](https://i.imgur.com/A0oN06c.png)\n",
    "<p style=\"text-align: center; font-size: 1em;\">\n",
    "**Locations of Edinburgh City Bike Counters**</p>\n",
    "\n",
    "It looks like we have a good selection of different types of locations, with \n",
    "*4 counters on commuting routes outside of Edinburgh (black markers)*,\n",
    "<span style=\"color:purple\">*6 counters in parks but which are also used for commuting*</span>,\n",
    "<span style=\"color:green\">*2 counters on canal paths*</span>,\n",
    "<span style=\"color:gray\">*18 counters on miscellaneous streets around Edinburgh*</span>, and\n",
    "<span style=\"color:blue\">*5 city center locations*</span>.\n",
    "\n",
    "There was no API so this data was downloaded in 48 csv files, each covering one counter:\n",
    "![Imgur](http://i.imgur.com/h9QygrN.png)\n",
    "\n",
    "Each file had the following structure:\n",
    "\n",
    "|counter_id|date      |time|channel_1|channel_2|channel_3|channel_4|  \n",
    "| -------- |:--------:|:--:|:-------:|:-------:|:-------:|:-------:|\n",
    "| 1        |18/03/2010|16  |12       |0        |0        |0        |\n",
    "| 2        |18/03/2010|17  |0        |9        |0        |0        |\n",
    "<p style=\"text-align: center; font-size: 1em;\">**Bike Counter CSV Structure**</p>\n",
    "\n",
    "Presumably, the channels refer to different lanes/directions, however we have no information detailing this. The channel columns do therefore not encode any specific information for our purposes and we will simply use combined totals in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.B) BIKE DATA STORAGE\n",
    "    \n",
    "Because of the fact that our data was originally in 48 different spreadsheets, the next step is to store the bike counter data into our mySQL database.\n",
    "\n",
    "We will first need to import the SQLAlchemy library and create a connection with the database:\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('mysql+pymysql://zcsah83:buciyutipa@128.40.150.34:3306/zcsah83')\n",
    "conn = engine\n",
    "```\n",
    "---\n",
    "We then create a table in which to store bike counter data:\n",
    "```python\n",
    "conn.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `edinburgh_CountersTable` (\n",
    "  `index` bigint(20) DEFAULT NULL,\n",
    "  `city` text(3) COLLATE utf8_bin,\n",
    "  `counter_id` bigint(20) DEFAULT NULL,\n",
    "  `date_time` datetime DEFAULT NULL,\n",
    "  `channel_1` bigint(20) DEFAULT NULL,\n",
    "  `channel_2` bigint(20) DEFAULT NULL,\n",
    "  `channel_3` bigint(20) DEFAULT NULL,\n",
    "  `channel_4` bigint(20) DEFAULT NULL,\n",
    "  `total` bigint(20) DEFAULT NULL,\n",
    "  unique (counter_id, date_time),\n",
    "  KEY `id_edinburgh_CountersTable` (`counter_id`)\n",
    ") ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\n",
    "\"\"\")\n",
    "```\n",
    "___\n",
    "and create an update function which we can call to compute total numbers of bikes per hour per counter:\n",
    "```python\n",
    "def updateTable():\n",
    "    conn.execute(\"\"\"\n",
    "    UPDATE edinburgh_CountersTable SET `total` = \n",
    "        IFNULL(`channel_1`,0) + IFNULL(`channel_2`,0)\n",
    "        + IFNULL(`channel_3`,0) + IFNULL(`channel_4`,0);\n",
    "    \"\"\")\n",
    "```\n",
    "___\n",
    "We then create a function to load individual bike counter CSVs into the `edinburgh_CountersTable`:<br/>\n",
    "*step 1:* Load the CSV into a Panda Table <br/>\n",
    "*step 2:* Use the .to_sql() function to load Panda Table into the Database (if the Data has already been loaded, this will throw up an *'integrity error'* due to the table constraints)<br/>\n",
    "*step 3:* Update SQL Table `total` column\n",
    "```python\n",
    "def loadCSV_ed(fileName):\n",
    "    \n",
    "    #step 1.\n",
    "    df=pd.read_csv(str(fileName),\n",
    "               parse_dates=[['date', 'time']],\n",
    "               dayfirst = True)\n",
    "    df['city']='EDI'\n",
    "    \n",
    "    #step 2.\n",
    "    df.to_sql('edinburgh_CountersTable', conn, flavor='sqlite', if_exists='append')\n",
    "    \n",
    "    #step 3.\n",
    "    updateTable()\n",
    "```\n",
    "---\n",
    "and use the `glob` module to loop through all the CSVs and load them into the database:\n",
    "```python\n",
    "import glob #http://docs.python.org/2/library/glob.html\n",
    "path = \"/Data Science for Spatial Systems/Project/Edinburgh Bike Counters/*.csv\"\n",
    "\n",
    "for csv in glob.glob(path):\n",
    "    loadCSV_ed(csv)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.C) BIKE DATA CONSISTENCY AND SAMPLE SELECTION\n",
    "\n",
    "We begin by querying the database for combined daily totals (for all counters), along with the number of active counters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 4))\n",
      "\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on '128.40.150.34' ([Errno 60] Operation timed out)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dcb529fb30a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mFROM\u001b[0m \u001b[0medinburgh_CountersTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \"\"\", conn)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnan_Days\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdailyTotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdailyTotals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             chunksize=chunksize)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pandas/io/sql.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;34m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnectable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m     def read_table(self, table_name, index_col=None, coerce_float=True,\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/engine/base.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \"\"\"\n\u001b[1;32m   1989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextual_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_with_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/engine/base.pyc\u001b[0m in \u001b[0;36mcontextual_connect\u001b[0;34m(self, close_with_result, **kwargs)\u001b[0m\n\u001b[1;32m   2037\u001b[0m         return self._connection_cls(\n\u001b[1;32m   2038\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2039\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_pool_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2040\u001b[0m             \u001b[0mclose_with_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose_with_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m             **kwargs)\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/engine/base.pyc\u001b[0m in \u001b[0;36m_wrap_pool_connect\u001b[0;34m(self, fn, connection)\u001b[0m\n\u001b[1;32m   2076\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 Connection._handle_dbapi_exception_noconnection(\n\u001b[0;32m-> 2078\u001b[0;31m                     e, dialect, self)\n\u001b[0m\u001b[1;32m   2079\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/engine/base.pyc\u001b[0m in \u001b[0;36m_handle_dbapi_exception_noconnection\u001b[0;34m(cls, e, dialect, engine)\u001b[0m\n\u001b[1;32m   1403\u001b[0m             util.raise_from_cause(\n\u001b[1;32m   1404\u001b[0m                 \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m                 \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1406\u001b[0m             )\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/util/compat.pyc\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpy3k\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/engine/base.pyc\u001b[0m in \u001b[0;36m_wrap_pool_connect\u001b[0;34m(self, fn, connection)\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0mdialect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2074\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2075\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \"\"\"\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_threadlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ConnectionFairy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36m_checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_checkout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadconns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfairy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfairy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0mfairy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mfairy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36mcheckout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheckout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mdbapi_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36m_do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dec_overflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m   \u001b[0;31m# remove potential circular references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy3k\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36m_do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inc_overflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;34m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ConnectionRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_invalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pool)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/pool.pyc\u001b[0m in \u001b[0;36m__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created new connection %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/engine/strategies.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m     95\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mcreator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'creator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/sqlalchemy/engine/default.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_connect_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pymysql/__init__.pyc\u001b[0m in \u001b[0;36mConnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \"\"\"\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymysql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconnections\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_orig_conn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pymysql/connections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, user, password, database, port, unix_socket, charset, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, ssl, read_default_group, compress, named_pipe, no_delay, autocommit, db, passwd, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_ssl_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msslp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dugald/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/pymysql/connections.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;31m# If e is neither DatabaseError or IOError, It's a bug.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on '128.40.150.34' ([Errno 60] Operation timed out)\")"
     ]
    }
   ],
   "source": [
    "dailyTotals = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT DATE(date_time) date, COUNT(counter_id)/24 as active_counters , SUM(total) as total_count \n",
    "    FROM edinburgh_CountersTable\n",
    "    GROUP BY date;\n",
    "    \"\"\", conn)\n",
    "\n",
    "nan_Days = dailyTotals[dailyTotals['total_count'].apply(np.isnan)].shape[0]\n",
    "\n",
    "x = dailyTotals ['date']\n",
    "y1 = dailyTotals ['total_count']\n",
    "y2 = dailyTotals ['active_counters']\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15,10))\n",
    "\n",
    "\n",
    "fig.suptitle('Daily Bike Counts & Number of Active Counters', fontsize=22)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x, y1, 'r-')\n",
    "ax2.plot(x, y2, 'b-', markersize=22)\n",
    "\n",
    "ax1.set_xlabel('date', fontsize=18)\n",
    "ax1.set_ylabel('daily bike count (all counters)', color='r', fontsize=18)\n",
    "ax2.set_ylabel('number of active counters', color='b', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the data does not appear to present any glaring errors of measurement, it is clear that all the counters are not always active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dailyTotals.plot(x='date', y='active_counters',\n",
    "                 fontsize=18,\n",
    "                 title='Number of Active Counters for Entire DataSet with Obvious Issues of Dependability of Some Counters',\n",
    "                 figsize=(15,4)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are obvious issues with counter activity / inactivity so we will try to understand which counters are on and which are off at any given time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter_activity = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT DATE(date_time) date, counter_id counter_activity, COUNT(counter_id)/24 as active_counters , SUM(total) as total_count \n",
    "    FROM edinburgh_CountersTable\n",
    "    GROUP BY counter_id, DATE(date_time);\n",
    "    \"\"\", conn)\n",
    "\n",
    "ax=counter_activity.plot(x='date', y='counter_activity',\n",
    "                           figsize=(15,12),\n",
    "                           yticks=range(1,49),\n",
    "                           style=\"b.\",\n",
    "                           grid=1,\n",
    "                           fontsize=16\n",
    "\n",
    "                          )\n",
    "ax.set(ylabel='counter id')\n",
    "ax.xaxis.label.set_size(18)\n",
    "ax.yaxis.label.set_size(18)\n",
    "plt.suptitle('Counter Activity / Inactivity Over Time', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows that none of the counters are active during the entirety of the time we are interested in.\n",
    "\n",
    "**However the *Counter Activity / Inactivity Over Time* plot suggests that we may have two samples of usable data:**\n",
    "\n",
    "-**sample 1 :** march 2011 to march 2013 with counters 1,2,(not 3),7,8,9,10,11,12,14,15,17,18,19,27,28 i.e. 15 counters\n",
    "\n",
    "-**sample 2 :** april 2015 to april 2016, using counters 20-48 (excluding counters 38 and 39 (Bruntsfield) as well as 23 (for which we have no data)). We will also need to be alert for what looks like potential breaks in our dataset.\n",
    "\n",
    "\n",
    "We will explore these samples further below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE 1\n",
    "march 2011 to march 2013\n",
    "![Imgur](http://i.imgur.com/TMXGsil.png)\n",
    "<p style=\"text-align: center; font-size: 1em;\">\n",
    "**Locations of Bike Counters Included in Sample 1**</p>\n",
    "\n",
    "Sample 1 consists of 15 counters covering many of the city's key bike commuting routes - Forth Road Bridge (9), Queensferry/Dalmeny (8), Cramond (17), Glasgow Road (14) RBS Gogar(10), Edinburgh Park Station (12), Union Canal West (18), Portobello Road(19), Water of Leith East (2), Haymarket Station(7), Peffermill Road (1) and Duddingston Road (11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter_activity_1 = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT DATE(date_time) date, counter_id counter_activity, COUNT(counter_id)/24 as active_counters , SUM(total) as total_count \n",
    "    FROM edinburgh_CountersTable\n",
    "    WHERE date_time >= '2011-03-01 00:00:00' AND date_time <= '2013-03-01 00:00:00'\n",
    "        AND counter_id IN (1,2,7,8,9,10,11,12,14,15,17,18,19,27,28) #counter selection\n",
    "    GROUP BY counter_id, DATE(date_time);\n",
    "    \"\"\", conn)\n",
    "\n",
    "ax=counter_activity_1.plot(x='date', y='counter_activity',\n",
    "                        c='red',\n",
    "                        figsize=(15, 12),\n",
    "                        yticks=range(1,49),\n",
    "                        style=\".\",\n",
    "                        grid=1,\n",
    "                        fontsize=16,\n",
    "                        title='Counter Activity for Sample 1 03-2014 to 03-2013 (15 counters)' )\n",
    "ax.set(ylabel='counter id')\n",
    "ax.yaxis.label.set_size(18)\n",
    "plt.show()\n",
    "\n",
    "counter_activity_1.plot(x='date', y='total_count',\n",
    "                        c='red', figsize=(15, 4),\n",
    "                        title='Daily Count for Sample 1 03-2014 to 03-2013 (15 counters)' \n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 1 looks consistent (counters are all active during entire period) with peaks in September due to the annual [Pedal for Scotland](http://pedalforscotland.org/history/) bike race.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAMPLE 2\n",
    "april 2015 to april 2016\n",
    "![Imgur](http://i.imgur.com/RrMdGHi.png)\n",
    "<p style=\"text-align: center; font-size: 1em;\">\n",
    "**Locations of Bike Counters Included in Sample 2**</p>\n",
    "\n",
    "Sample 2 consists of 26 counters with little overlap with sample 1.\n",
    "\n",
    "The only location present in both samples is Portobello Road which corresponds to counter_id 19 in sample 1 and counter_id 20 in sample 2.\n",
    "\n",
    "Otherwise, sample 2 is made up of commuting routes - Costorphine Road (29), Queensferry Road (35), Raeburn Place (36), Crewe Road (48) B901 (28 and 32) London Road (33), Water of Leith West (24), Stenhouse Drive (26), Dalry Road (30), Union Canal Central (27), Inverleith Park (47), Dundee Street (31), Morrison Street (21), Nicholson Street (34), Mayfield Road (40), Fishwives Causeway (46), and 5 bike counters on the Meadows / Bruntsfield Links (22, 37, 42, 43, 44, 45)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "counter_activity_2 = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT DATE(date_time) date, counter_id counter_activity, COUNT(counter_id)/24 as active_counters , SUM(total) as total_count \n",
    "    FROM edinburgh_CountersTable\n",
    "    WHERE date_time >= '2015-04-00 00:00:00'  #time selection\n",
    "        AND counter_id >= 20 AND counter_id NOT IN (23, 38, 39) #counter selection\n",
    "    GROUP BY counter_id, DATE(date_time);\n",
    "    \"\"\", conn)\n",
    "\n",
    "ax=counter_activity_2.plot(x='date', y='counter_activity',\n",
    "                        c='blue',\n",
    "                        figsize=(15, 12),\n",
    "                        yticks=range(1,49),\n",
    "                        style=\".\",\n",
    "                        grid=1,\n",
    "                        fontsize=16,\n",
    "                        title='Counter Activity for Sample 2 04-2015 to 04-2016 (26 counters)' )\n",
    "ax.set(ylabel='counter id')\n",
    "ax.yaxis.label.set_size(18)\n",
    "plt.show()\n",
    "\n",
    "counter_activity_2.plot(x='date', y='total_count',\n",
    "                        c='blue', figsize=(15, 4),\n",
    "                        title='Daily Count for Sample 2 04-2015 to 04-2016 (26 counters)' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, sample 2 is far more inconsistent than sample 2 (the counters present numerous periods of inactivity).\n",
    "\n",
    "Given the lack of spatial overlap between samples, it will not be possible to put the samples together.\n",
    "\n",
    "**Therefore, contingent on having weather data for the period, it looks like sample 1 of our bike data (march 2011 to march 2013 with counters 1,2,7,8,9,10,11,12,14,15,17,18,19,27,28) will be a better fit for our analysis.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='II. WEATHER DATA, STORAGE, CLEANING, CONSISTENCY AND SAMPLE SELECTION'></a>\n",
    "# II. WEATHER DATA, STORAGE, CLEANING, CONSISTENCY AND SAMPLE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.A) THE WEATHER DATA\n",
    "    \n",
    "Minute by minute historical weather data was downloaded from the [University of Edinburgh's weather station site](http://www.ed.ac.uk/geosciences/weather-station). CSVs from 2011 through to 2015 were available, however 2016 and 2017 weather data could not be obtained.\n",
    "\n",
    "| datetime|atmostpheric pressure|rainfall|wind speed|wind direction|temperature|humidity|solar flux|battery|        \n",
    "| ------- |:-------------------------:|:---------:|\n",
    "<p style=\"text-align: center; font-size: 1em;\">\n",
    "**Historical Weather Data CSV Structure**</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.B) WEATHER DATA STORAGE\n",
    "\n",
    "We will also store the weather data in our mysql database.\n",
    "\n",
    "Again, we create a table in which to store our historical weather data (in this case 2015):\n",
    "```python\n",
    "conn.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `ed_weather15` (\n",
    "  `index` bigint(20) NOT  NULL,\n",
    "  `datetime` datetime DEFAULT NULL COMMENT 'min',\n",
    "  `rainfall` decimal(5,2) DEFAULT NULL COMMENT 'mm',\n",
    "  `wind` decimal(5,3) DEFAULT NULL COMMENT 'metres per second',\n",
    "  `temp` decimal(5,3) DEFAULT NULL COMMENT 'C',\n",
    "  `sun` decimal(10,10) DEFAULT NULL COMMENT 'Kw/m2',\n",
    "  KEY `ix_ed_weather15_DateTime` (`datetime`)\n",
    ") ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;\n",
    "\"\"\")\n",
    "```\n",
    "___\n",
    "Making sure that our CSV have datetime formatted as `YYYY-MM-DD hh:mm:ss`,\n",
    "\n",
    "a. we read in the csv to pandas dataframe:<br/>\n",
    "```python\n",
    "df15=pd.read_csv('JCMB_2015.csv'),\n",
    "```\n",
    "\n",
    "b. rename the columns to avoid spaces:<br/>\n",
    "```python\n",
    "df15.columns = ['datetime',\n",
    "               'rainfall',\n",
    "               'wind',\n",
    "               'temp',\n",
    "               'sun'],\n",
    "```\n",
    " \n",
    "c. check for empty Values:<br/>\n",
    "```python\n",
    "nan = df15[df15['rainfall'].apply(np.isnan)].shape[0]\n",
    "print'NaN values Rainfall: ', nan\n",
    "nan_min = df15[df15['wind'].apply(np.isnan)].shape[0]\n",
    "print'NaN values Wind Speed: ', nan\n",
    "nan_min = df15[df15['temp'].apply(np.isnan)].shape[0]\n",
    "print'NaN values Temp: ', nan\n",
    "nan_min = df15[df15['sun'].apply(np.isnan)].shape[0]\n",
    "print'NaN values Solar flux: ', nan\n",
    "```\n",
    "\n",
    "d. and finally, we load the dataframe into our database:<br/>\n",
    "```python\n",
    "df15.to_sql('ed_weather15', conn, flavor='sqlite', if_exists='append')```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.C) WEATHER DATA CLEANING\n",
    "\n",
    "As is often the case, the weather data has some issues, the next step we take is to check for and replace **NaN values** as well as **measurement errors**, we also read the weather station [logbook](http://www.ed.ac.uk/geosciences/weather-station) to check for any logged malfunctions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.D) WEATHER DATA CONSISTENCY AND SAMPLE SELECTION\n",
    "\n",
    "We have already selected the period of bike counter data we are looking to use, we plot weather data availability against bike counter activity - the previously selected bike counter sample period is highlighted in pink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql(\n",
    "    '''\n",
    "SELECT t1.date, t1.counter_activity, t2.weather_availability\n",
    "FROM(\n",
    "\tSELECT DATE(date_time) date, counter_id counter_activity\n",
    "\tFROM edinburgh_CountersTable WHERE DATE(date_time) >= '2007-10-01 00:00:00'\n",
    "\tGROUP BY counter_id, date) t1\n",
    "LEFT JOIN(\n",
    "\tSELECT DATE(datetime) date, ROUND( COUNT(DATE(datetime))/1440, 0) weather_availability FROM ed_weather11 GROUP BY date\n",
    "\t\tUNION ALL\n",
    "\tSELECT DATE(datetime) date, ROUND( COUNT(DATE(datetime))/1440, 0) weather_availability FROM ed_weather12 GROUP BY date\n",
    "\t\tUNION ALL\n",
    "\tSELECT DATE(datetime) date, ROUND( COUNT(DATE(datetime))/1440, 0) weather_availability FROM ed_weather13 GROUP BY date\n",
    "\t\tUNION ALL\n",
    "\tSELECT DATE(datetime) date, ROUND( COUNT(DATE(datetime))/1440, 0) weather_availability FROM ed_weather14 GROUP BY date\n",
    "\t\tUNION ALL\n",
    "\tSELECT DATE(datetime) date, ROUND( COUNT(DATE(datetime))/1440, 0) weather_availability FROM ed_weather15 GROUP BY date\n",
    "\t) t2\n",
    "ON t1.date = t2.date;\n",
    "    ''', conn)\n",
    "\n",
    "x = df['date']\n",
    "y1 = df['counter_activity']\n",
    "y2 = df['weather_availability']\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15,10))\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "fig.suptitle('Weather Data Availability (blue), Bike Counter Activity/Inactivity (grey) - sample 1 period highlighted in pink', fontsize=22)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(x, y1, '.', c='grey')\n",
    "ax2.plot(x, y2, 'b.', markersize=24)\n",
    "\n",
    "ax1.xaxis.set_tick_params(labelsize=16)\n",
    "ax1.yaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "ax1.set_ylim([0,30])\n",
    "ax2.set_ylim([0,100])\n",
    "\n",
    "ax1.set_yticks(range(0,31))\n",
    "ax2.set_yticks([0,0])\n",
    "\n",
    "ax1.grid('-', linewidth=0.2)\n",
    "\n",
    "ax1.set_xlabel('date', fontsize=18)\n",
    "ax1.set_ylabel('bike counter id', color='grey', fontsize=18)\n",
    "ax2.set_ylabel('weather data availability', color='blue', rotation=0, fontsize=18)\n",
    "ax2.get_yaxis().set_label_coords(1.2,0.02)\n",
    "\n",
    "plt.axvspan('10 Mar 2011', 'March 2013', color='red', alpha=0.05) #sammple 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excellent, weather data is available during *most* of the period we have selected. However, we will need to omit the period from January to March 2013 from our data. We will therefore use march 2011 to december 2012 as our sample period.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='III. DATA ANALYSIS'></a>\n",
    "# III. DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.A) PRELIMINARY DATA ANALYSIS\n",
    "\n",
    "Our working hypothesis is that when weather conditions deteriorate, bike use decreases.\n",
    "We will first try to verify this visually by plotting Daily Bike Counts against Rainfall, Maximum Wind and Temperature over time (due to the fact that our data will display considerable seasonality, plotting Bike use against an individual variable may not be particularly meaningful). We begin by writing a select statement to pull out our data for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data  = pd.read_sql(\n",
    "    '''\n",
    "SELECT\n",
    "\tt1.date, t1.weekday, t1.dayofyear,\n",
    "\tt1.total_count,\n",
    "\tROUND( t1.active_counters, 0) active_counters,\n",
    "\tIFNULL(t2.rainfall, 0) rainfall, #8 NULL values \n",
    "\tROUND( IFNULL(t2.avg_wind, 0) ,3) avg_wind, #8 NULL values\n",
    "\tIFNULL(t2.max_wind, 0) max_wind, #8 NULL values\n",
    "\tROUND( IFNULL(t2.avg_temp, 0) ,3) avg_temp, #there are no NULL values\n",
    "    IFNULL(t2.min_temp, 0) min_temp, #there are no NULL values\n",
    "\tIFNULL(t2.max_temp, 0) max_temp, #there are no NULL values\n",
    "\tROUND( IFNULL(t2.sun, 0), 3) sun, #8 NULL values\n",
    "\tROUND( IFNULL(t2.max_sun, 0) ,3) max_sunhour #8 NULL values\n",
    "FROM\n",
    "\t(\n",
    "\tSELECT DATE(date_time) date, WEEKDAY(date_time) weekday, DAYOFYEAR(date_time) dayofyear, SUM(total) total_count, COUNT(counter_id)/24 active_counters\n",
    "\tFROM edinburgh_CountersTable WHERE date_time >= '2011-03-01 00:00:00' AND date_time <= '2012-12-31  23:59:59'\n",
    "\tGROUP BY date\n",
    "\t) t1\n",
    "LEFT JOIN\n",
    "\t(\n",
    "\tSELECT DATE(datetime) date, SUM(rainfall) rainfall, AVG(wind) avg_wind, MAX(wind) max_wind,\n",
    "    AVG(temp) avg_temp, MIN(temp) min_temp, MAX(temp) max_temp, SUM(sun) sun, MAX(sun) max_sun\n",
    "\tFROM ed_weather11 WHERE datetime >= '2011-03-01 00:00:00' AND datetime <= '2012-12-31  23:59:59'\n",
    "\tGROUP BY date\n",
    "\tUNION ALL\n",
    "\tSELECT DATE(datetime) date, SUM(rainfall) rainfall, AVG(wind) avg_wind, MAX(wind) max_wind,\n",
    "    AVG(temp) avg_temp, MIN(temp) min_temp, MAX(temp) max_temp, SUM(sun) sun, MAX(sun) max_sun\n",
    "\tFROM ed_weather12 WHERE datetime >= '2011-03-01 00:00:00' AND datetime <= '2012-12-31  23:59:59'\n",
    "\tGROUP BY date\n",
    "\tUNION ALL\n",
    "\tSELECT DATE(datetime) date, SUM(rainfall) rainfall, AVG(wind) avg_wind, MAX(wind) max_wind,\n",
    "    AVG(temp) avg_temp, MIN(temp) min_temp, MAX(temp) max_temp, SUM(sun) sun, MAX(sun) max_sun\n",
    "\tFROM ed_weather13 WHERE datetime >= '2011-03-01 00:00:00' AND datetime <= '2012-12-31  23:59:59'\n",
    "\tGROUP BY date\n",
    "\t) t2\n",
    "ON t1.date = t2.date;\n",
    "    ''', conn)\n",
    "\n",
    "data['date']=pd.to_datetime(data['date'])\n",
    "data.index = data['date'].values\n",
    "\n",
    "x = data['date']\n",
    "y0 = data['total_count']\n",
    "y1 = data['rainfall']\n",
    "y2 = data['max_wind']\n",
    "y3 = data['avg_temp']\n",
    "\n",
    "fig, ax0 = plt.subplots(figsize=(15,17))\n",
    "\n",
    "fig.suptitle('Daily Bike Counts, Rainfall (mm), Maximum Wind (km/h) and Temperature (C) over Sample Period', fontsize=26)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "ax0.set_xlabel('date', color='grey', fontsize=25)\n",
    "ax0.xaxis.set_tick_params(labelsize=16)\n",
    "\n",
    "#daily bike count\n",
    "ax0.plot(x, y0, '-', color='black', alpha=0.4, lineWidth=5)\n",
    "ax0.set_ylabel('daily bike count', color='black', alpha=0.4, fontsize=25)\n",
    "ax0.tick_params(colors='grey')\n",
    "ax0.yaxis.set_tick_params(labelsize=25)\n",
    "ax0.set_ylim([0, 15000])\n",
    "\n",
    "#daily rainfall\n",
    "ax1 = ax0.twinx()\n",
    "ax1.plot(x, y1, '-', color='b', alpha=1, lineWidth=2)\n",
    "ax1.set_ylabel('rainfall (mm)', color='b', fontsize=20)\n",
    "ax1.yaxis.set_tick_params(labelsize=16)\n",
    "ax1.tick_params(colors='b')\n",
    "ax1.set_ylim([0, 200])\n",
    "\n",
    "#daily max wind\n",
    "ax2 = ax0.twinx()\n",
    "ax2.plot(x, y2, '^', color='green', alpha=1.4, markerSize=7)\n",
    "ax2.set_ylabel('max gusts (km/h)', color='green', fontsize=20)\n",
    "ax2.yaxis.set_tick_params(labelsize=16)\n",
    "ax2.set_ylim([0, 100])\n",
    "ax2.tick_params(colors='green')\n",
    "ax2.spines['right'].set_position(('axes', 1.1))\n",
    "\n",
    "#daily average temperature\n",
    "ax3 = ax0.twinx()\n",
    "ax3.plot(x, y3, '-', color='red', alpha=0.5, lineWidth=3)\n",
    "ax3.set_ylabel('temperature (C)', color='red', fontsize=20)\n",
    "ax3.yaxis.set_tick_params(labelsize=16)\n",
    "ax3.tick_params(colors='red')\n",
    "ax3.spines['right'].set_position(('axes', 1.2))\n",
    "ax3.set_ylim([-10, 50])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph seems to confirm our hypothesis that the weather has a significant effect on bike usage,\n",
    "The graph clearly shows the close relationship between average temperature and daily bike counts.\n",
    "The inverse relationship between daily maximum wind speed and bike counts is also clearly visible.\n",
    "\n",
    "However, the relationships evidenced by this chart can be qualified as seasonal / longer-term, we will now plot a shorter period of time to try and understand the impact of short-term weather changes on bike usage.\n",
    "\n",
    "The period between the beginning of August and the end of October 2011 looks like it is characterised by significant variations in rainfall, along with some stormy weather, we will therefore analysis this period in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataShort= data[datetime.date(2011,8,1) : datetime.date(2011,10,31)]\n",
    "\n",
    "x = dataShort['date']\n",
    "y0 = dataShort['total_count']\n",
    "y1 = dataShort['rainfall']\n",
    "y2 = dataShort['max_wind']\n",
    "y3 = dataShort['avg_temp']\n",
    "\n",
    "fig, ax0 = plt.subplots(figsize=(15,17))\n",
    "\n",
    "fig.suptitle('Daily Bike Counts, Rainfall (mm), Maximum Wind (km/h) and Temperature (C) over a Shorter Period (1st August 2011 to 31st October 2011)',\n",
    "             fontsize=26)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "ax0.set_xlabel('date', color='grey', fontsize=25)\n",
    "ax0.xaxis.set_tick_params(labelsize=18)\n",
    "\n",
    "#daily bike count\n",
    "ax0.plot(x, y0, '-', color='black', alpha=0.4, lineWidth=8)\n",
    "ax0.set_ylabel('daily bike count', color='black', alpha=0.4, fontsize=25)\n",
    "ax0.tick_params(colors='grey')\n",
    "ax0.yaxis.set_tick_params(labelsize=25)\n",
    "ax0.set_ylim([0, 15000])\n",
    "\n",
    "#daily rainfall\n",
    "ax1 = ax0.twinx()\n",
    "ax1.plot(x, y1, '-', color='b', alpha=1, lineWidth=2)\n",
    "ax1.set_ylabel('rainfall (mm)', color='b')\n",
    "ax1.tick_params(colors='b')\n",
    "ax1.yaxis.set_tick_params(labelsize=16)\n",
    "ax1.set_ylim([0, 140])\n",
    "\n",
    "#daily max wind\n",
    "ax2 = ax0.twinx()\n",
    "ax2.plot(x, y2, '-', color='green', alpha=1.4, lineWidth=3)\n",
    "ax2.set_ylabel('max gusts (km/h)', color='green')\n",
    "ax2.set_ylim([0, 100])\n",
    "ax2.yaxis.set_tick_params(labelsize=16)\n",
    "ax2.tick_params(colors='green')\n",
    "ax2.spines['right'].set_position(('axes', 1.1))\n",
    "\n",
    "#daily average temperature\n",
    "ax3 = ax0.twinx()\n",
    "ax3.plot(x, y3, '-', color='red', alpha=0.5, lineWidth=3)\n",
    "ax3.set_ylabel('temperature (C)', color='red')\n",
    "ax3.yaxis.set_tick_params(labelsize=16)\n",
    "ax3.tick_params(colors='red')\n",
    "ax3.spines['right'].set_position(('axes', 1.2))\n",
    "ax3.set_ylim([-10, 50])\n",
    "\n",
    "#highlight significant areas\n",
    "plt.axvspan(datetime.date(2011,8,4), datetime.date(2011,8,8), color='blue', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,8,9), datetime.date(2011,8,13), color='blue', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,8,18), datetime.date(2011,8,21), color='green', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,8,26), datetime.date(2011,8,30), color='gray', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,9,4), datetime.date(2011,9,8), color='gray', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,9,10), datetime.date(2011,9,12), color='orange', alpha=0.5)\n",
    "plt.axvspan(datetime.date(2011,9,12), datetime.date(2011,9,15), color='gray', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,9,26), datetime.date(2011,10,1), color='red', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,10,1), datetime.date(2011,10,3), color='blue', alpha=0.1)\n",
    "plt.axvspan(datetime.date(2011,10,15), datetime.date(2011,10,19), color='gray', alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above shorter term plot, it is clear that <span style=\"color:blue\">daily rainfall maxima correlate with bike count minima **(periods highlighted in blue)**</span>, <span style=\"color:green\">wind maxima correlate with bike count minima **(in green)**</span>, <span style=\"color:gray\">high rainfall and high winds correlate with low bike counts **(in grey)**</span>, <span style=\"color:red\">, while temperature peaks correlate with bike count peaks **(in red)**</span>.\n",
    "\n",
    "*Once again, we observe the spike caused by the Pedal for Scotland Event held on the 11th of September in 2011<span style=\"color:orange\">**(in orange)**</span>, this will need to be dealt with in our analysis.*\n",
    "\n",
    "**Our bike traffic data seems to exhibit a broad pattern of seasonal trends combined with short-term variations hypothesised to be responsive to weather changes. An Autoregressive Moving Average model with Exogenous Variables (ARIMAX) - a time series based model may be a good choice for making bike count predictions.<p/>We will also build a linear predicitive models with scikit learn and compare performance.<p/>These different models all make predictions based on past events, therefore to build them, we will first need to decide which bit of the past we are interested by choosing time lags. We will do this using the autocorrelation function.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.B) SEASONALITY AND TIME LAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries): #adapted from https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window=12,center=False).mean()\n",
    "    rolstd = timeseries.rolling(window=12,center=False).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print 'Results of Dickey-Fuller Test:'\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print dfoutput\n",
    "\n",
    "test_stationarity(data['total_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Dickey-Fuller test show that we cannot reject the hypothesis that our data is non-stationary (i.e. not characterised by a constant mean, variance and autocovariance that does not depend on time).\n",
    "\n",
    "In order to choose which Time Lags to use, we will first use *first order differencing* to try and remove seasonality from the data before plotting the Autocorrelation Function on the differenced data. First order differencing involves taking the difference between observations at a given t with the observation of a previous t-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_log = np.log(data['total_count'])\n",
    "print 'Log transform taken.'\n",
    "\n",
    "ts_log_diff = ts_log - ts_log.shift(1)\n",
    "print 'First order differencing.'\n",
    "\n",
    "ts_log_diff.dropna(inplace=True)#it's important to make sure there are no null values.\n",
    "test_stationarity(ts_log_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of first order differencing seems to have considerably improved our data's stationarity, we can now reject the hypothesis that the first order differenced data is non-stationary with 99% confidence.\n",
    "\n",
    "We will now use the autocorrelation function (a measure of the correlation between the TS with a lagged version of itself) on this modified data to chose the time lags we will employ; plotting the ACF function for a decreasing numbers of lags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Import autocorrelation function'\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "lag_acf = acf(ts_log_diff, nlags=700)\n",
    "plt.subplot(121)\n",
    "plt.title('Autocorrelation Function up to 700 lags', fontsize=16)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "\n",
    "lag_acf = acf(ts_log_diff, nlags=300)\n",
    "plt.subplot(122)\n",
    "plt.title('Autocorrelation Function up to 300 lags', fontsize=16)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lag_acf = acf(ts_log_diff, nlags=32)\n",
    "plt.subplot(121)\n",
    "plt.title('Autocorrelation Function up to 32 lags', fontsize=16)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.xticks([0,1,2,3,4,5,7,14,21,28,31])\n",
    "plt.grid('-', linewidth=0.5)\n",
    "\n",
    "lag_acf = acf(ts_log_diff, nlags=16)\n",
    "plt.subplot(122)\n",
    "plt.title('Autocorrelation Function up to 16 lags', fontsize=16)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
    "plt.xticks(range(0,16))\n",
    "plt.grid('-', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-order differencing has obviously not done anything to eliminate the weekly patterns in our data. Our autocorrelation function suggests that the following lags: t-7, t-14, t-21, t-28 will all be useful in order to make predictions. This weekly pattern is somewhat intuitive but we can confirm this by decomposing the data's seasonal trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.plot(seasonal_decompose(ts_log).seasonal,label='Seasonality')\n",
    "plt.legend(loc='best', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='IV. FORECASTING WITH SCIKIT LEARN'></a>\n",
    "# IV. FORECASTING WITH SCIKIT LEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "\n",
    "Our first model aimed at forecasting daily bike counts will be a linear model.\n",
    "\n",
    "It should be highlighted that, in theory our data presents autocorrelation and therefore violates a key assumption of linear regression. However, in practice, our model should still be able to make reasonable **short-term** forecasts, with issues due to autocorrelation only appearing for longer range forecasting.\n",
    "\n",
    "We begin by pulling in our data for forecasting from our database, adding in a number of dummy variables (presence of rain) and scaling variables such as wind and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_sql(\n",
    "    '''\n",
    "SELECT\n",
    "\tt1.date, \n",
    "\tt1.total_count,\n",
    "\tROUND( t1.active_counters, 0) active_counters,\n",
    "\tt1.weekday, t1.dayofyear, t1.month, t1.season_, t1.uni_,\n",
    "\n",
    "\tIFNULL(t2.rainfall, 0) rainfall, #8 NULL values \n",
    "\tt2.rain_BIN_, #Binary rain variable\n",
    "\tROUND( IFNULL(t2.avg_wind, 0) ,3) avg_wind, #8 NULL values\n",
    "\tIFNULL(t2.max_wind, 0) max_wind, #8 NULL values\n",
    "\tt2.wind_, t2.wind_gust_, #average wind variable scaled, #max wind variable scaled\n",
    "\tROUND( IFNULL(t2.avg_temp, 0) ,3) avg_temp, #there are no NULL values\n",
    "    \tt2.temp_, t2.mintemp_, #average temperature variable scaled #min temperature scaled\n",
    "    \tIFNULL(t2.min_temp, 0) min_temp, #there are no NULL values\n",
    "\tIFNULL(t2.max_temp, 0) max_temp, #there are no NULL values\n",
    "\tROUND( IFNULL(t2.sun, 0), 3) sun, #8 NULL values\n",
    "\tROUND( IFNULL(t2.max_sun, 0) ,3) max_sunhour #8 NULL values\n",
    "FROM\n",
    "\t(\n",
    "\tSELECT\n",
    "\t\tDATE(date_time) date, DAYOFYEAR(date_time) dayofyear, WEEKDAY(date_time) weekday, MONTH(date_time) month,\n",
    "\t\tCASE #SEASONAL dummy variable 'season_'\n",
    "\t\t\tWHEN MONTH(date_time) IN (3,4,5) THEN 1 #spring\n",
    "    \t\t\tWHEN MONTH(date_time) IN (6,7,8) THEN 2 #summer\n",
    "    \t\t\tWHEN MONTH(date_time) IN (9,10,11) THEN 3 #autumn\n",
    "    \t\t\tELSE 0 #winter\n",
    "   \t\tEND AS 'season_',\n",
    "   \t\tCASE #UNIVERSITY dummy variable 'uni_'\n",
    "   \t\t\tWHEN MONTH(date_time) IN (1,6,7,8) THEN 0 ELSE 1 #0: uni holidays, 1:uni term time\n",
    "   \t\tEND AS 'uni_', \n",
    "\t\tSUM(total) total_count, COUNT(counter_id)/24 active_counters\n",
    "\tFROM edinburgh_CountersTable\n",
    "\t\tWHERE date_time >= '2011-03-01 00:00:00' AND date_time <= '2012-12-31  23:59:59'\n",
    "\t\t\tAND counter_id IN (1,2,7,8,9,10,11,12,14,15,17,18,19,27,28)\n",
    "\t\tGROUP BY date\n",
    "\t) t1\n",
    "LEFT JOIN\n",
    "\t(\n",
    "\tSELECT\n",
    "\t\tDATE(datetime) date,\n",
    "    \t\tSUM(rainfall) rainfall,\n",
    "    \t\tCASE #RAIN BINARY 'rain_bin_'\n",
    "      \t\tWHEN SUM(rainfall)= 0 THEN 0 #no rain\n",
    "     \t\telse 1 #rain\n",
    "    \t\tEND AS 'rain_BIN_',\n",
    "    \t\tAVG(wind) avg_wind, MAX(wind) max_wind,\n",
    "    \t\tCASE  #WIND scaled dummy variable 'wind_'\n",
    "      \t\tWHEN AVG(wind) <= 3.3 THEN 0 #no wind\n",
    "      \t\tWHEN AVG(wind) <= 5.5 THEN 1 #light winds\n",
    "      \t\tWHEN AVG(wind) <= 10.7 THEN 2 #moderate winds\n",
    "      \t\tWHEN AVG(wind) <= 17.1 THEN 3 #high winds\n",
    "      \t\tWHEN AVG(wind) > 17.1 THEN 4 #gale\n",
    "    \t\tEND AS 'wind_',\n",
    "    \t\tCASE #WIND GUSTING scaled variable wind_gust_\n",
    "      \t\tWHEN MAX(wind) <= 3.3 THEN 0 #no wind\n",
    "      \t\tWHEN MAX(wind) <= 5.5 THEN 1 #light winds\n",
    "      \t\tWHEN MAX(wind) <= 10.7 THEN 2 #moderate winds\n",
    "      \t\tWHEN MAX(wind) <= 17.1 THEN 3 #high winds\n",
    "      \t\tWHEN MAX(wind) > 17.1 THEN 4 #gale\n",
    "    \t\tEND AS 'wind_gust_',\n",
    "    \t\tAVG(temp) avg_temp, MIN(temp) min_temp, MAX(temp) max_temp,\n",
    "    \t\tCASE \n",
    "      \t\tWHEN AVG(temp)<= 0 THEN 0 #freezing\n",
    "      \t\tWHEN AVG(temp)< 5 THEN 1 # very cold\n",
    "      \t\tWHEN AVG(temp)< 10 THEN 2 #cold\n",
    "      \t\tWHEN AVG(temp)< 15 THEN 3 #moderate\n",
    "      \t\tWHEN AVG(temp)< 20 THEN 4 #warm\n",
    "      \t\tWHEN AVG(temp)>= 20 THEN 5 #hot\n",
    "    \t\tEND AS 'temp_',\n",
    "    \t\tCASE #MIN TEMPERATURE scaled variable 'mintemp_'\n",
    "      \t\tWHEN MIN(temp)<= 0 THEN 0 #freezing\n",
    "      \t\tWHEN MIN(temp)< 5 THEN 1 #very cold\n",
    "      \t\tWHEN MIN(temp)< 10 THEN 2 #cold\n",
    "      \t\tWHEN MIN(temp)< 15 THEN 3 #moderate\n",
    "      \t\tWHEN MIN(temp)< 20 THEN 4 #warm\n",
    "      \t\tWHEN MIN(temp)>= 20 THEN 5 #hot\n",
    "    \t\tEND AS 'mintemp_',\n",
    "    \t\tSUM(sun) sun, MAX(sun) max_sun\n",
    "    \tFROM ed_weather11\n",
    "    \t\tWHERE datetime >= '2011-03-01 00:00:00' AND datetime <= '2012-12-31  23:59:59'\n",
    "    \t\tGROUP BY date\n",
    "    \t\t\n",
    "   UNION ALL\n",
    "   \n",
    "   \tSELECT\n",
    "\t\tDATE(datetime) date,\n",
    "    \t\tSUM(rainfall) rainfall,\n",
    "    \t\tCASE #RAIN BINARY 'rain_bin_'\n",
    "      \t\tWHEN SUM(rainfall)= 0 THEN 0 #no rain\n",
    "     \t\telse 1 #rain\n",
    "    \t\tEND AS 'rain_BIN_',\n",
    "    \t\tAVG(wind) avg_wind, MAX(wind) max_wind,\n",
    "    \t\tCASE  #WIND scaled dummy variable 'wind_'\n",
    "      \t\tWHEN AVG(wind) <= 3.3 THEN 0 #no wind\n",
    "      \t\tWHEN AVG(wind) <= 5.5 THEN 1 #light winds\n",
    "      \t\tWHEN AVG(wind) <= 10.7 THEN 2 #moderate winds\n",
    "      \t\tWHEN AVG(wind) <= 17.1 THEN 3 #high winds\n",
    "      \t\tWHEN AVG(wind) > 17.1 THEN 4 #gale\n",
    "    \t\tEND AS 'wind_',\n",
    "    \t\tCASE #WIND GUSTING scaled variable wind_gust_\n",
    "      \t\tWHEN MAX(wind) <= 3.3 THEN 0 #no wind\n",
    "      \t\tWHEN MAX(wind) <= 5.5 THEN 1 #light winds\n",
    "      \t\tWHEN MAX(wind) <= 10.7 THEN 2 #moderate winds\n",
    "      \t\tWHEN MAX(wind) <= 17.1 THEN 3 #high winds\n",
    "      \t\tWHEN MAX(wind) > 17.1 THEN 4 #gale\n",
    "    \t\tEND AS 'wind_gust_',\n",
    "    \t\tAVG(temp) avg_temp, MIN(temp) min_temp, MAX(temp) max_temp,\n",
    "    \t\tCASE \n",
    "      \t\tWHEN AVG(temp)<= 0 THEN 0 #freezing\n",
    "      \t\tWHEN AVG(temp)< 5 THEN 1 # very cold\n",
    "      \t\tWHEN AVG(temp)< 10 THEN 2 #cold\n",
    "      \t\tWHEN AVG(temp)< 15 THEN 3 #moderate\n",
    "      \t\tWHEN AVG(temp)< 20 THEN 4 #warm\n",
    "      \t\tWHEN AVG(temp)>= 20 THEN 5 #hot\n",
    "    \t\tEND AS 'temp_',\n",
    "    \t\tCASE #MIN TEMPERATURE scaled variable 'mintemp_'\n",
    "      \t\tWHEN MIN(temp)<= 0 THEN 0 #freezing\n",
    "      \t\tWHEN MIN(temp)< 5 THEN 1 #very cold\n",
    "      \t\tWHEN MIN(temp)< 10 THEN 2 #cold\n",
    "      \t\tWHEN MIN(temp)< 15 THEN 3 #moderate\n",
    "      \t\tWHEN MIN(temp)< 20 THEN 4 #warm\n",
    "      \t\tWHEN MIN(temp)>= 20 THEN 5 #hot\n",
    "    \t\tEND AS 'mintemp_',\n",
    "    \t\tSUM(sun) sun, MAX(sun) max_sun\n",
    "    \tFROM ed_weather12\n",
    "    \t\tWHERE datetime >= '2011-03-01 00:00:00' AND datetime <= '2012-12-31  23:59:59'\n",
    "    \t\tGROUP BY date\t\n",
    "\t) t2\n",
    "ON t1.date = t2.date;\n",
    "    ''', conn)\n",
    "\n",
    "data1['date']=pd.to_datetime(data1['date'])\n",
    "\n",
    "df1=data1[[\n",
    "        'date','total_count', 'weekday', 'dayofyear',\n",
    "        'month', 'season_', 'uni_',\n",
    "        'rainfall', 'rain_BIN_', \n",
    "        'wind_', 'wind_gust_', \n",
    "        'avg_temp', 'temp_', 'min_temp', 'mintemp_', 'max_temp'\n",
    "        ]]\n",
    "df1.rename(columns={'total_count': 'y'}, inplace=True)\n",
    "df1['date']=pd.to_datetime(df1['date'])\n",
    "df1.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that we would need to deal with the yearly bike count spikes due to the [Pedal for Scotland](http://pedalforscotland.org/history/) race on the 11th September 2011 and 9th September 2012:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def colorRow(s):\n",
    "    return ['background-color: #cd0000' if i==len(s)-1 else '' for i in range(len(s))]\n",
    "\n",
    "display(\n",
    "df1[(df1['date'] >= '2011-09-10') & (df1['date'] <= '2011-09-11')].style.apply(colorRow),\n",
    "df1[(df1['date'] >= '2012-09-08') & (df1['date'] <= '2012-09-09')].style.apply(colorRow),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we will be forecasting based on lagged values, deleting these rows will have a significant effect on the amount of data we have to work with, as such we will simply subtract the number of participants that took part in the Pedal for Scotland event in each of those years *(8500 in 2011 and 7600 in 2012) from their respective bike counts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.set_value(194, 'y', 11829-8500)\n",
    "df1.set_value(558, 'y', 13036-7600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a function to generate lagged data and use it to create a dataset which includes lagged data going back as far as 21 days (t-1 ,t-2 ,t-7 ,t-15 ,t-21):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lagData(addTo, data, lag):\n",
    "\n",
    "    tempLag = data.shift(lag)\n",
    "\n",
    "    oldCol = ['date',\n",
    "        'y',\n",
    "        'weekday', 'dayofyear', 'month', 'season_', 'uni_',\n",
    "        'rainfall', 'rain_BIN_', \n",
    "        'wind_', 'wind_gust_', \n",
    "        'avg_temp', 'temp_', 'min_temp', 'mintemp_', 'max_temp']\n",
    "    \n",
    "    newCol = ['date-'+str(lag),\n",
    "        'y-'+str(lag),\n",
    "        'weekday-'+str(lag), 'dayofyear-'+str(lag), 'month-'+str(lag),\n",
    "        'season_-'+str(lag), 'uni_-'+str(lag),\n",
    "        'rainfall-'+str(lag), 'rain_BIN_-'+str(lag), \n",
    "        'wind_-'+str(lag), 'wind_gust_-'+str(lag), \n",
    "        'avg_temp-'+str(lag), 'temp_-'+str(lag), 'min_temp-'+str(lag), 'mintemp_-'+str(lag), 'max_temp-'+str(lag)]\n",
    "\n",
    "    tempLag.rename(columns=dict(zip(oldCol, newCol)), inplace=True); del tempLag['date-'+str(lag)]\n",
    "\n",
    "    addTo = addTo.join(tempLag)\n",
    "    return addTo\n",
    "\n",
    "d1 = df1 # create d: dataframe with lags \n",
    "\n",
    "d1 = lagData(d1, df1, 1)\n",
    "d1 = lagData(d1, df1, 2)\n",
    "d1 = lagData(d1, df1, 7)\n",
    "d1 = lagData(d1, df1, 15)\n",
    "d1 = lagData(d1, df1, 21)\n",
    "\n",
    "#d1.index = d1['date'].values #make date the index\n",
    "d1 = d1.dropna() # drop the null values e.g. 21 rows with no -21 lagged values\n",
    "\n",
    "data = d1\n",
    "data.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(data.describe().iloc[0:3,0:15].drop(['weekday','month','season_','dayofyear','uni_'],1))\n",
    "printmd('<center>**Data Summary**</center>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = data['date']\n",
    "y0 = data['y']\n",
    "y1 = data['rainfall']\n",
    "y3 = data['avg_temp']\n",
    "\n",
    "fig, ax0 = plt.subplots(figsize=(15,17))\n",
    "\n",
    "fig.suptitle('''Daily Bike Counts, Rainfall (mm), Maximum Wind (km/h) and Temperature (C) over Sample Period\n",
    "             (Data in Blue will be used for training our model, Data in Red for Testing)''', fontsize=26)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "ax0.set_xlabel('date', color='grey', fontsize=25)\n",
    "ax0.xaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "#daily bike count\n",
    "ax0.plot(x, y0, '-', color='black', alpha=0.4, lineWidth=5)\n",
    "ax0.set_ylabel('daily bike count', color='black', alpha=0.4, fontsize=25)\n",
    "ax0.tick_params(colors='grey')\n",
    "ax0.yaxis.set_tick_params(labelsize=25)\n",
    "ax0.set_ylim([0, 10000])\n",
    "\n",
    "#daily rainfall\n",
    "ax1 = ax0.twinx()\n",
    "ax1.plot(x, y1, '-', color='b', alpha=1, lineWidth=2)\n",
    "ax1.set_ylabel('rainfall (mm)', color='b')\n",
    "ax1.tick_params(colors='b')\n",
    "ax1.set_ylim([0, 150])\n",
    "\n",
    "#daily average temperature\n",
    "ax3 = ax0.twinx()\n",
    "ax3.plot(x, y3, '-', color='red', alpha=0.5, lineWidth=3)\n",
    "ax3.set_ylabel('temperature (C)', color='red')\n",
    "ax3.tick_params(colors='red')\n",
    "ax3.spines['right'].set_position(('axes', 1.2))\n",
    "ax3.set_ylim([-10, 30])\n",
    "\n",
    "#highlight area for training\n",
    "plt.axvspan(datetime.date(2011,3,20), datetime.date(2012,6,25), color='blue', alpha=0.2)\n",
    "#highlight area for prediction\n",
    "plt.axvspan(datetime.date(2012,6,25), datetime.date(2012,12,31), color='red', alpha=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are interested in predicting bike counts at least a week in advance, we drop the bike count lagged by 1 and 2 days ( `y-1` and `y-2` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data['y-1']\n",
    "del data['y-2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset consisting of daily bike counts, counts for the same day for the prior 3 weeks, a number of date variables (weekday, day of year, month, season, university term) and weather variables on the day as well as lagged for the previous 3 days and for the same day in the 3 weeks prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(data.head(0))\n",
    "printmd('<center>**Structure of Data For Prediction**</center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the bike counts and explanatory variables and we split the data into training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=data['y']\n",
    "X=data.drop(['y','date'],1)\n",
    "\n",
    "train_x = X.loc[0:450]\n",
    "train_y = Y.loc[0:450]\n",
    "\n",
    "test_x = X.loc[451:501]\n",
    "test_y = Y.loc[451:501]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have setup our data in a way that mimics short-term forecasting of bike counts with weather predictions, we then write a function to fit and test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, title, nbResults):\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    \n",
    "    var=pd.DataFrame(X.columns)\n",
    "    var.rename(columns={var.columns[0]:'variable'}, inplace=True)\n",
    "    coef=pd.DataFrame(model.coef_)\n",
    "    coef.rename(columns={coef.columns[0]:'coef'}, inplace=True)\n",
    "    modelCoef= pd.concat([var,coef], axis=1)\n",
    "    display(modelCoef.head(14)) #uncomment to display coefficients\n",
    "\n",
    "    predictions = pd.DataFrame(model.predict(test_x) )\n",
    "    trueVal = pd.DataFrame(test_y)\n",
    "\n",
    "    test=data[['date','weekday','rain_BIN_','rainfall','avg_temp','wind_gust_','y']].loc[451:501]\n",
    "    test['day']=range(51)\n",
    "\n",
    "    predictions.rename(columns={predictions.columns[0]:'y_pred'}, inplace=True)\n",
    "    predictions['day']=range(51)\n",
    "    results=test.merge(predictions, on='day')\n",
    "    \n",
    "    \n",
    "    results['difference']=results['y_pred']-results['y']\n",
    "    results['PE']=(results['y']-results['y_pred'])/results['y_pred']*100\n",
    "    results['APE']=abs((results['y']-results['y_pred'])/results['y_pred']*100)\n",
    "\n",
    "    x =  results['date']\n",
    "    y1 = results['y']\n",
    "    y2 = results['y_pred']\n",
    "    \n",
    "    y3 = test['rainfall']\n",
    "    y4 = test['avg_temp']\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(15,10))\n",
    "\n",
    "    fig.suptitle(title+' Predictions (yellow dash) vs True Values (grey)', fontsize=22)\n",
    "\n",
    "    ax2 = ax1\n",
    "    ax1.plot(x, y1, '-', color='gray', lineWidth=6, alpha=0.5)\n",
    "    ax2.plot(x, y2, 'y--', lineWidth=10)\n",
    "\n",
    "    ax1.set_xticks(pd.date_range(datetime.date(2012,5,25),datetime.date(2012,12,31)),5)\n",
    "    ax1.grid('-', linewidth=0.2)\n",
    "\n",
    "    ax1.xaxis_date()\n",
    "    ax1.set_xlabel('''date''')\n",
    "    ax2.set_ylabel('daily bike counts')\n",
    "    ax2.set_ylim([0, 10000])\n",
    "    \n",
    "    #daily rainfall\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.plot(x, y3, '-', color='b', alpha=1, lineWidth=2)\n",
    "    ax3.set_ylabel('rainfall (mm)', color='b')\n",
    "    ax3.tick_params(colors='b')\n",
    "    ax3.set_ylim([0, 150])\n",
    "\n",
    "    #daily average temperature\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.plot(x, y4, '-', color='red', alpha=0.5, lineWidth=3)\n",
    "    ax4.set_ylabel('temperature (C)', color='red')\n",
    "    ax4.tick_params(colors='red')\n",
    "    ax4.spines['right'].set_position(('axes', 1.2))\n",
    "    ax4.set_ylim([-10, 30])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    del results['day']\n",
    "    display(results.head(nbResults)) #uncomment to display dataframe of predictions\n",
    "    \n",
    "    print 'mean square error (MSE)', np.mean((model.predict(test_x)-test_y)**2)\n",
    "    print 'explained variance', model.score(test_x, test_y)\n",
    "    print 'mean percentage error (MPE)', results[['PE']].mean()[0],'%'\n",
    "    print 'mean absolute percentage error (MAPE)', results[['APE']].mean()[0],'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "predict(model ,'Linear Regression Model', 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results forecasted using the linear model closely resemble the true values, with near identical trends. The above graph suggests the model is relatively good at predicting the results of heavy rainfall.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='V. FORECASTING WITH ARIMAX'></a>\n",
    "# V. FORECASTING WITH ARIMAX\n",
    "\n",
    "We will build our ARIMAX model (Autoregressive Integrated Moving Averages with exogenous variables) using a similar process to generate training and testing data aimed at simulating a short term bike demand forecast (7 days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyflux as pf\n",
    "from datetime import datetime\n",
    "print 'import PyFlux and datetime'\n",
    "\n",
    "df1['date']=pd.to_datetime(df1['date'])\n",
    "train = df1.loc[0:450]\n",
    "train.index = train['date'].values\n",
    "test = df1.loc[451:501]\n",
    "test.index = test['date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit our ARIMAX model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = pf.ARIMAX(data=train, formula='y~1+weekday+month+season_+rainfall+avg_temp+wind_gust_+uni_',\n",
    "                  ar=1, ma=1, family=pf.Normal())\n",
    "x = model.fit(\"MLE\")\n",
    "print '\\t\\t\\t\\t\\tARIMAX with Exogenous Variables'\n",
    "x.summary()\n",
    "print '\\n\\t\\t\\t\\t\\tFitted Model'\n",
    "model.plot_fit(figsize=(13,6), fontsize=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model and original data are a very close fit (we may have overfitted our model), we then make a 7 day forecast beginning on the 25th of may 2012:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.plot_predict(h=7, oos_data=test, past_values=50, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and write a function to check the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ARIMAXpredict(h):\n",
    "    predictions = model.predict(h, test, intervals=False)\n",
    "    predictions.rename(columns={predictions.columns[0]:'y_pred'}, inplace=True)\n",
    "    predictions['day']=range(h)\n",
    "    testN=test[:h]\n",
    "    testN['day']=range(h)\n",
    "    results = testN.merge(predictions, on='day')\n",
    "    results=results[['date','weekday','rain_BIN_','rainfall','avg_temp','wind_gust_','y','y_pred']]\n",
    "    results['difference']=results['y_pred']-results['y']\n",
    "    results['PE']=(results['y']-results['y_pred'])/results['y_pred']*100\n",
    "    results['APE']=abs((results['y']-results['y_pred'])/results['y_pred']*100)\n",
    "    display(results.head(10))\n",
    "    \n",
    "    resultsPlot= results[['date','y','y_pred']]\n",
    "    resultsPlot.index = resultsPlot['date'].values\n",
    "    resultsPlot.plot(title='ARIMAX Predictions for '+str(h)+' days (green) vs True Values (blue)')\n",
    "    \n",
    "    print 'mean square error (MSE)', np.mean((results['y_pred']-results['y'])**2)\n",
    "    print 'mean percentage error (MPE)', results[['PE']].mean()[0],'%'\n",
    "    print 'mean absolute percentage error (MAPE)', results[['APE']].mean()[0],'%'\n",
    "    \n",
    "ARIMAXpredict(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This looks relatively similar in accuracy to our linear model. The mean percentage error (MPE) is of -1.53% compared to -3.00% for our linear model while the mean absolute percentage error (MAPE) is of 9.64%  compared to 14.93% for our linear model.** However, it is clear that our model is not properly accounting for the underlying patterns in our data, this is especially obvious if we plot a longer forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ARIMAXpredict(h):\n",
    "    predictions = model.predict(h, test, intervals=False)\n",
    "    predictions.rename(columns={predictions.columns[0]:'y_pred'}, inplace=True)\n",
    "    predictions['day']=range(h)\n",
    "    testN=test[:h]\n",
    "    testN['day']=range(h)\n",
    "    results = testN.merge(predictions, on='day')\n",
    "    results=results[['date','weekday','rain_BIN_','rainfall','avg_temp','wind_gust_','y','y_pred']]\n",
    "    results['difference']=results['y_pred']-results['y']\n",
    "    results['PE']=(results['y']-results['y_pred'])/results['y_pred']*100\n",
    "    results['APE']=abs((results['y']-results['y_pred'])/results['y_pred']*100)\n",
    "    \n",
    "    resultsPlot= results[['date','y','y_pred']]\n",
    "    resultsPlot.index = resultsPlot['date'].values\n",
    "    resultsPlot.plot(title='ARIMAX Predictions for '+str(h)+' days (green) vs True Values (blue)')\n",
    "\n",
    "ARIMAXpredict(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It should be said, that a number of other attempts were made at building an ARIMAX model accounting for the seasonality present in our data, including multiple order differencing (logarithmic and non-logarithmic), setting different numbers of auto-regressive terms (AR) and moving average terms (MA) but we were unable to obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion, if we are looking to use our model to predict what will happen as a result of bad weather, it seems that our linear model performs better than our ARIMAX model:\n",
    "Thursday the 31st of May 2012 is a good example of this, a day of high rainfall led to a real drop of around 2,900 cyclists compared to the previous day which on a weekday would likely indicate an increase in people taking other forms of public transport. The linear model's prediction was off by 500 (overestimating by +10%) while the ARIMAX prediction was off by over 1,000 (overestimating by +20%).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DISCUSSION'></a>\n",
    "# DISCUSSION\n",
    "\n",
    "These models confirm that short term weather changes have a significant effect on bicycle traffic. Our models were not able to fully explain variation in daily bike counts, this is undoubtedly due to other factors at play as well as patterns of serial correlation revealed in our exploration of seasonality which we failed to properly account for.\n",
    "\n",
    "This area of work is interesting as more accurate models will enable precisely calibrated increases in the provision of services such as buses and trains on days of forecasted high rainfall and/or wind and low temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Toggle ON/OFF the Input Code\"></form>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
